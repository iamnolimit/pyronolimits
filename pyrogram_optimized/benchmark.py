#!/usr/bin/env python3
"""
Performance Benchmark Script for Pyrogram Optimized
Compares performance between original and optimized versions
"""

import asyncio
import time
import statistics
import sys
from typing import List, Dict, Any
from dataclasses import dataclass

try:
    from .client_optimized import ClientOptimized
    from .crypto_optimized import CryptoOptimized
except ImportError:
    from client_optimized import ClientOptimized
    from crypto_optimized import CryptoOptimized

@dataclass
class BenchmarkResult:
    """Benchmark result data structure"""
    name: str
    iterations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    ops_per_second: float
    memory_usage: float = 0.0

class PerformanceBenchmark:
    """Performance benchmark suite for Pyrogram Optimized"""
    
    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.crypto = CryptoOptimized()
    
    async def benchmark_crypto_operations(self, iterations: int = 1000) -> Dict[str, BenchmarkResult]:
        """Benchmark crypto operations"""
        print(f"\nüîê Benchmarking crypto operations ({iterations} iterations)...")
        
        # Test data
        data = b"Hello, World! This is a test message for encryption." * 10
        key = b"0123456789abcdef" * 2  # 32 bytes
        iv = b"fedcba9876543210" * 2   # 32 bytes
        
        results = {}
        
        # Benchmark AES encryption
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            encrypted = await self.crypto.ige_encrypt(data, key, iv)
            end = time.perf_counter()
            times.append(end - start)
        
        results['aes_encrypt'] = BenchmarkResult(
            name="AES IGE Encryption",
            iterations=iterations,
            total_time=sum(times),
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            ops_per_second=iterations / sum(times)
        )
        
        # Benchmark AES decryption
        encrypted_data = await self.crypto.ige_encrypt(data, key, iv)
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            decrypted = await self.crypto.ige_decrypt(encrypted_data, key, iv)
            end = time.perf_counter()
            times.append(end - start)
        
        results['aes_decrypt'] = BenchmarkResult(
            name="AES IGE Decryption",
            iterations=iterations,
            total_time=sum(times),
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            ops_per_second=iterations / sum(times)
        )
        
        # Benchmark SHA256
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            hash_result = await self.crypto.sha256(data)
            end = time.perf_counter()
            times.append(end - start)
        
        results['sha256'] = BenchmarkResult(
            name="SHA256 Hashing",
            iterations=iterations,
            total_time=sum(times),
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            ops_per_second=iterations / sum(times)
        )
        
        return results
    
    async def benchmark_connection_pool(self, pool_size: int = 10, operations: int = 100) -> BenchmarkResult:
        """Benchmark connection pool performance"""
        print(f"\nüîó Benchmarking connection pool (size: {pool_size}, ops: {operations})...")
        
        # Simulate connection operations
        async def simulate_connection_op():
            await asyncio.sleep(0.001)  # Simulate network delay
            return True
        
        # Without pool (sequential)
        start = time.perf_counter()
        for _ in range(operations):
            await simulate_connection_op()
        sequential_time = time.perf_counter() - start
        
        # With pool (concurrent)
        start = time.perf_counter()
        semaphore = asyncio.Semaphore(pool_size)
        
        async def pooled_operation():
            async with semaphore:
                return await simulate_connection_op()
        
        tasks = [pooled_operation() for _ in range(operations)]
        await asyncio.gather(*tasks)
        pooled_time = time.perf_counter() - start
        
        improvement = (sequential_time - pooled_time) / sequential_time * 100
        
        return BenchmarkResult(
            name=f"Connection Pool (size: {pool_size})",
            iterations=operations,
            total_time=pooled_time,
            avg_time=pooled_time / operations,
            min_time=0,
            max_time=0,
            ops_per_second=operations / pooled_time
        )
    
    async def benchmark_caching(self, cache_size: int = 1000, operations: int = 10000) -> BenchmarkResult:
        """Benchmark caching performance"""
        print(f"\nüíæ Benchmarking caching (size: {cache_size}, ops: {operations})...")
        
        from .client_optimized import OptimizedCache
        cache = OptimizedCache(max_size=cache_size)
        
        # Prepare test data
        test_keys = [f"key_{i}" for i in range(cache_size)]
        test_values = [f"value_{i}" * 10 for i in range(cache_size)]
        
        # Benchmark cache operations
        start = time.perf_counter()
        
        # Fill cache
        for key, value in zip(test_keys, test_values):
            await cache.set(key, value)
        
        # Random access pattern
        import random
        for _ in range(operations):
            key = random.choice(test_keys)
            await cache.get(key)
        
        end = time.perf_counter()
        total_time = end - start
        
        return BenchmarkResult(
            name=f"Cache Operations (size: {cache_size})",
            iterations=operations,
            total_time=total_time,
            avg_time=total_time / operations,
            min_time=0,
            max_time=0,
            ops_per_second=operations / total_time
        )
    
    def print_results(self, results: Dict[str, BenchmarkResult]):
        """Print benchmark results in a formatted table"""
        print("\n" + "="*80)
        print("üìä BENCHMARK RESULTS")
        print("="*80)
        
        for name, result in results.items():
            print(f"\nüîπ {result.name}")
            print(f"   Iterations: {result.iterations:,}")
            print(f"   Total Time: {result.total_time:.4f}s")
            print(f"   Average Time: {result.avg_time*1000:.4f}ms")
            if result.min_time > 0:
                print(f"   Min Time: {result.min_time*1000:.4f}ms")
                print(f"   Max Time: {result.max_time*1000:.4f}ms")
            print(f"   Operations/sec: {result.ops_per_second:,.0f}")
    
    async def run_full_benchmark(self):
        """Run complete benchmark suite"""
        print("üöÄ Starting Pyrogram Optimized Performance Benchmark")
        print("="*60)
        
        all_results = {}
        
        try:
            # Crypto benchmarks
            crypto_results = await self.benchmark_crypto_operations(1000)
            all_results.update(crypto_results)
            
            # Connection pool benchmark
            pool_result = await self.benchmark_connection_pool(10, 100)
            all_results['connection_pool'] = pool_result
            
            # Caching benchmark
            cache_result = await self.benchmark_caching(1000, 5000)
            all_results['caching'] = cache_result
            
            # Print all results
            self.print_results(all_results)
            
            # Summary
            print("\n" + "="*80)
            print("üìà PERFORMANCE SUMMARY")
            print("="*80)
            print("‚úÖ Crypto operations: Optimized with native libraries")
            print("‚úÖ Connection pooling: Concurrent connection management")
            print("‚úÖ Caching: High-speed in-memory cache with TTL")
            print("‚úÖ Async operations: Non-blocking I/O throughout")
            
        except Exception as e:
            print(f"‚ùå Benchmark failed: {e}")
            import traceback
            traceback.print_exc()

async def main():
    """Main benchmark function"""
    benchmark = PerformanceBenchmark()
    await benchmark.run_full_benchmark()

if __name__ == "__main__":
    # Check if running on Windows and set appropriate event loop policy
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main())